[{"authors":null,"categories":null,"content":"I\u0026rsquo;m one of the founding team members of the Baidu Autonomous Driving Car project. I joined Baidu in 2014. I\u0026rsquo;m the principal architect of Baidu Autonomous Driving Technology Department (ADT) now. Since Jan. 2016, I am the tech lead of the mapping and localization team at Baidu ADT.\nMy research interests include Computer Vision, Robotics, Simultaneous Localization and Mapping (SLAM), Structure from Motion (SFM), 3D Reconstruction.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://songshiyu01.github.io/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"I\u0026rsquo;m one of the founding team members of the Baidu Autonomous Driving Car project. I joined Baidu in 2014. I\u0026rsquo;m the principal architect of Baidu Autonomous Driving Technology Department (ADT) now. Since Jan. 2016, I am the tech lead of the mapping and localization team at Baidu ADT.\nMy research interests include Computer Vision, Robotics, Simultaneous Localization and Mapping (SLAM), Structure from Motion (SFM), 3D Reconstruction.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://songshiyu01.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":["Weixin Lu","Guowei Wan","Yao Zhou","Xiangyu Fu","Pengfei Yuan","**Shiyu Song**"],"categories":null,"content":"","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"a9c0b8bce3b12ee840ca76c78f520866","permalink":"https://songshiyu01.github.io/publication/arxiv2019_registration/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/publication/arxiv2019_registration/","section":"publication","summary":"We present DeepICP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the inference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. Our loss function incorporates both the local similarity and the global geometric constraints to ensure all above network designs can converge towards the right direction. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable or better performance than the state-of-the-art geometry-based methods. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method makes it attractive for substantial applications relying on the point cloud registration task.","tags":[],"title":"DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration","type":"publication"},{"authors":["Weixin Lu","Yao Zhou","Guowei Wan","Shenhua Hou","**Shiyu Song**"],"categories":null,"content":"","date":1550304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550304000,"objectID":"73891adf2dc74faa0824701c122a911b","permalink":"https://songshiyu01.github.io/publication/cvpr2019_localization/","publishdate":"2019-02-16T00:00:00-08:00","relpermalink":"/publication/cvpr2019_localization/","section":"publication","summary":"We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The  SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation.","tags":[],"title":"L3-Net: Towards Learning based LiDAR Localization for Autonomous Driving","type":"publication"},{"authors":["Shiyu Song"],"categories":null,"content":"","date":1529251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529251200,"objectID":"d2a70ef2fb8054f480715815d3e5c82b","permalink":"https://songshiyu01.github.io/talk/cvpr2019/","publishdate":"2018-06-17T09:00:00-07:00","relpermalink":"/talk/cvpr2019/","section":"talk","summary":"We make a brief introduction of the techniques behind our multi-sensor fusion based localization system. We present a robust and precise localization system that achieves centimeter-level localization accuracy in disparate city scenes. The system adaptively uses information from complementary sensors such as GNSS, LiDAR and IMU to achieve high localization accuracy and resilience in challenging scenes, such as urban downtown, highways, and tunnels. In this tutorial, we introduce the technical principles of each individual localization method and the multisensor fusion framework. We also cover our latest work in the exploration of the learning based LiDAR localization method. It's good for engineers and Ph.D. students who are interested in the vehicle localization system.","tags":[],"title":"Inside Apollo: Multisensor Fusion Based Localization","type":"talk"},{"authors":["Guowei Wan","Xiaolong Yang","Renlan Cai","Hao Li","Yao Zhou","Hao Wang","**Shiyu Song**"],"categories":null,"content":"","date":1526886000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526886000,"objectID":"e43ca0370610bcda7aa22f0e55aded83","permalink":"https://songshiyu01.github.io/publication/icra2018_localization/","publishdate":"2018-05-21T00:00:00-07:00","relpermalink":"/publication/icra2018_localization/","section":"publication","summary":"We present a robust and precise localization system that achieves centimeter-level localization accuracy in disparate city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and resilience in challenging scenes, such as urban downtown, highways, and tunnels. Rather than relying only on LiDAR intensity or 3D geometry, we make innovative use of LiDAR intensity and altitude cues to significantly improve localization system accuracy and robustness. Our GNSS RTK module utilizes the help of the multi-sensor fusion framework and achieves a better ambiguity resolution success rate. An error-state Kalman filter is applied to fuse the localization measurements from different sources with novel uncertainty estimation. We validate, in detail, the effectiveness of our approaches, achieving 5-10cm RMS accuracy and outperforming previous state-of-the-art systems. Importantly, our system, while deployed in a large autonomous driving fleet, made our vehicles fully autonomous in crowded city streets despite road construction that occurred from time to time. A dataset including more than 60 km real traffic driving in various urban roads is used to comprehensively test our system.","tags":[],"title":"Robust and Precise Vehicle Localization based on Multi-sensor Fusion in Diverse City Scenes","type":"publication"},{"authors":["Shiyu Song"],"categories":null,"content":"","date":1522252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522252800,"objectID":"8c0ed1ac311006869dbf0d9d7dc58950","permalink":"https://songshiyu01.github.io/talk/baiduai2018/","publishdate":"2018-03-28T09:00:00-07:00","relpermalink":"/talk/baiduai2018/","section":"talk","summary":"We make brief introduction of HD Map services at Baidu Apollo platform and the techniques behind our multi-sensor fusion based localization system.","tags":[],"title":"Localization and HD Map at Baidu IDG","type":"talk"},{"authors":["Shiyu Song"],"categories":null,"content":"","date":1522252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522252800,"objectID":"3f53f6a4cc7cb01ff941fd1eb4bfbabc","permalink":"https://songshiyu01.github.io/talk/mipr2019/","publishdate":"2018-03-28T09:00:00-07:00","relpermalink":"/talk/mipr2019/","section":"talk","summary":"We present a robust and precise localization system that achieves centimeter-level localization accuracy in varied city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and robustness in various challenging scenes, including urban downtown, highway, tunnel and so on. Moving forward, we introduce our latest work in exploring learning-based localization system. It leverages the help of the deep neural network structures to establish a learning-based approach that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines.","tags":[],"title":"Towards Learning-based Localization for Autonomous Driving","type":"talk"},{"authors":["Guowei Wan","Xiaolong Yang","Renlan Cai","Hao Li","Hao Wang","**Shiyu Song**"],"categories":null,"content":"","date":1510704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510704000,"objectID":"70fdf56c3712678e988f052f5b942a7b","permalink":"https://songshiyu01.github.io/publication/arxiv2017_localization/","publishdate":"2017-11-15T00:00:00Z","relpermalink":"/publication/arxiv2017_localization/","section":"publication","summary":"We present a robust and precise localization system that achieves centimeter-level localization accuracy in disparate city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and resilience in challenging scenes, such as urban downtown, highways, and tunnels. Rather than relying only on LiDAR intensity or 3D geometry, we make innovative use of LiDAR intensity and altitude cues to significantly improve localization system accuracy and robustness. Our GNSS RTK module utilizes the help of the multi-sensor fusion framework and achieves a better ambiguity resolution success rate. An error-state Kalman filter is applied to fuse the localization measurements from different sources with novel uncertainty estimation. We validate, in detail, the effectiveness of our approaches, achieving 5-10cm RMS accuracy and outperforming previous state-of-the-art systems. Importantly, our system, while deployed in a large autonomous driving fleet, made our vehicles fully autonomous in crowded city streets despite road construction that occurred from time to time. A dataset including more than 60 km real traffic driving in various urban roads is used to comprehensively test our system.","tags":[],"title":"Robust and Precise Vehicle Localization based on Multi-sensor Fusion in Diverse City Scenes","type":"publication"},{"authors":["Shiyu Song"],"categories":null,"content":"","date":1505836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505836800,"objectID":"e394d4612a32e2833cb299bc17eac536","permalink":"https://songshiyu01.github.io/talk/isprs2017/","publishdate":"2017-09-19T09:00:00-07:00","relpermalink":"/talk/isprs2017/","section":"talk","summary":"We make brief introduction of the development history of Baidu self-driving car, Baidu Apollo platform for developers, the techniques behind the Baidu HD Map and our multi-sensor fusion based localization system. We present a robust and precise localization system that achieves centimeter-level localization accuracy in varied city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and robustness in various challenging scenes, including urban downtown, highway, tunnel and so on. Both our HD Map products and localization system have been deployed in a large autonomous driving fleet, and make our vehicles fully autonomous in crowded city streets every day.","tags":[],"title":"HD Map, Localization and Self-Driving Car","type":"talk"},{"authors":["**Shiyu Song**","Manmohan Chandraker","Clark C. Guest"],"categories":null,"content":"","date":1459494000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459494000,"objectID":"8222257a391e42a7b036b6427b4d9fac","permalink":"https://songshiyu01.github.io/publication/pami2016_monovo/","publishdate":"2016-04-01T00:00:00-07:00","relpermalink":"/publication/pami2016_monovo/","section":"publication","summary":"We present a real-time monocular visual odometry system that achieves high accuracy in real-world autonomous driving applications. First, we demonstrate robust monocular SFM that exploits multithreading to handle driving scenes with large motions and rapidly changing imagery. To correct for scale drift, we use known height of the camera from the ground plane. Our second contribution is a novel data-driven mechanism for cue combination that allows highly accurate ground plane estimation by adapting observation covariances of multiple cues, such as sparse feature matching and dense inter-frame stereo, based on their relative confidences inferred from visual data on a per-frame basis. Finally, we demonstrate extensive benchmark performance and comparisons on the challenging KITTI dataset, achieving accuracy comparable to stereo and exceeding prior monocular systems. Our SFM system is optimized to output pose within 50 ms in the worst case, while average case operation is over 30 fps. Our framework also significantly boosts the accuracy of applications like object localization that rely on the ground plane.","tags":[],"title":"High Accuracy Monocular SFM and Scale Correction for Autonomous Driving","type":"publication"},{"authors":null,"categories":null,"content":" Summary We present a robust and precise localization system that achieves centimeter-level localization accuracy in disparate city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and resilience in challenging scenes, such as urban downtown, highways, and tunnels. Rather than relying only on LiDAR intensity or 3D geometry, we make innovative use of LiDAR intensity and altitude cues to significantly improve localization system accuracy and robustness. Our GNSS RTK module utilizes the help of the multi-sensor fusion framework and achieves a better ambiguity resolution success rate. An error-state Kalman filter is applied to fuse the localization measurements from different sources with novel uncertainty estimation. We validate, in detail, the effectiveness of our approaches, achieving 5-10cm RMS accuracy and outperforming previous state-of-the-art systems. Importantly, our system, while deployed in a large autonomous driving fleet, made our vehicles fully autonomous in crowded city streets despite road construction that occurred from time to time. A dataset including more than 60 km real traffic driving in various urban roads is used to comprehensively test our system.\nSensors Our autonomous vehicle is equipped with a Velodyne LiDAR HDL-64E. An integrated navigation system, NovAtel ProPak6 plus NovAtel IMU-IGM-A1, is installed for raw sensor data collection, such as GNSS pseudo range and carrier wave, IMU specific force and rotation rate. The built-in tightly integrated inertial and satellite navigation solution was not used. A computing platform equipped with Dual Xeon E5-2658 v3 12 cores, and a Xilinx KU115 FPGA chip with 55% utilization for LiDAR localization.\nFramework Overview of the architecture of our system that estimates the optimal position, velocity, attitude (PVA) of the autonomous vehicle using a loosely coupled error-state-Kalman filter. It combines sensor input (purple) with pre-built LiDAR map (yellow). GNSS and LiDAR estimate the PVA used by an error-state Kalman filter as the measurements, while the Kalman filter provides the predicted prior PVA. The strap-down inertial navigation system (SINS) is used as a prediction model in the Kalman filter propagation phase by integrating the specific force $f^b$ measured by the accelerometer and the rotation rate $\\omega_{ib}^b$ measured by the gyroscope. The corrections including the bias of accelerometer and gyroscope, the errors of PVA, etc estimated by the Kalman filter are fed to the SINS.\nAccuracy Our system has been extensively tested in real-world driving scenarios. We compare our localization performance against the state-of-the-art intensity-based localization method proposed by Levinson et al. [12] and the built-in tightly-coupled GNSS/IMU integrated solution in the commercial product. In order to explicitly demonstrate the contribution of different sensors, the test results of our proposed system are shown in two modes:\n 2-Systems: LiDAR + IMU 3-Systems: LiDAR + GNSS + IMU.  In the table, we show the quantitative results in both regular or weak GNSS roads. Note our vast performance improvement over [12] and the robust and accurate localization results in both regular and weak GNSS scenarios with centimeter level accuracy.\nVideos These videos provide a brief description of the project and demonstrate the performance of the localization system.\n  ","date":1446451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446451200,"objectID":"1ed2ad44af3b68ef41bff90310904192","permalink":"https://songshiyu01.github.io/project/msf_localization/","publishdate":"2015-11-02T00:00:00-08:00","relpermalink":"/project/msf_localization/","section":"project","summary":"A robust and precise vehicle localization system that achieves centimeter-level accuracy by adaptively fusing information from multiple complementary sensors, such as GNSS, LiDAR, camera and IMU, for self-driving cars.","tags":[],"title":"Multi-sensor Fusion based Localization System","type":"project"},{"authors":["**Shiyu Song**","Manmohan Chandraker"],"categories":null,"content":"","date":1433746800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433746800,"objectID":"ee736fb4cd1dc4c981028c1136d44b52","permalink":"https://songshiyu01.github.io/publication/cvpr2015_object/","publishdate":"2015-06-08T00:00:00-07:00","relpermalink":"/publication/cvpr2015_object/","section":"publication","summary":"We present a system for fast and highly accurate 3D localization of objects like cars in autonomous driving applications, using a single camera. Our localization framework jointly uses information from complementary modalities such as structure from motion (SFM) and object detection to achieve high localization accuracy in both near and far fields. This is in contrast to prior works that rely purely on detector outputs, or motion segmentation based on sparse feature tracks. Rather than completely commit to tracklets generated by a 2D tracker, we make novel use of raw detection scores to allow our 3D bounding boxes to adapt to better quality 3D cues. To extract SFM cues, we demonstrate the advantages of dense tracking over sparse mechanisms in autonomous driving scenarios. In contrast to complex scene understanding, our formulation for 3D localization is efficient and can be regarded as an extension of sparse bundle adjustment to incorporate object detection cues. Experiments on the KITTI dataset show the efficacy of our cues, as well as the accuracy and robustness of our 3D object localization relative to ground truth and prior works.","tags":[],"title":"Joint SFM and Detection Cues for Monocular 3D Localization in Road Scenes","type":"publication"},{"authors":["**Shiyu Song**","Manmohan Chandraker"],"categories":null,"content":"","date":1403593200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1403593200,"objectID":"a835ccaaf5aa697a0a0715fe2f30592a","permalink":"https://songshiyu01.github.io/publication/cvpr2014_monovo/","publishdate":"2014-06-24T00:00:00-07:00","relpermalink":"/publication/cvpr2014_monovo/","section":"publication","summary":"Scale drift is a crucial challenge for monocular autonomous driving to emulate the performance of stereo. This paper presents a real-time monocular SFM system that corrects for scale drift using a novel cue combination framework for ground plane estimation, yielding accuracy comparable to stereo over long driving sequences. Our ground plane estimation uses multiple cues like sparse features, dense inter-frame stereo and (when applicable) object detection. A data-driven mechanism is proposed to learn models from training data that relate observation covariances for each cue to error behavior of its underlying variables. During testing, this allows per-frame adaptation of observation covariances based on relative confidences inferred from visual data. Our framework significantly boosts not only the accuracy of monocular self-localization, but also that of applications like object localization that rely on the ground plane. Experiments on the KITTI dataset demonstrate the accuracy of our ground plane estimation, monocular SFM and object localization relative to ground truth, with detailed comparisons to prior art.","tags":[],"title":"Robust Scale Estimation in Real-Time Monocular SFM for Autonomous Driving","type":"publication"},{"authors":["**Shiyu Song**","Manmohan Chandraker","Clark C. Guest"],"categories":null,"content":"","date":1367823600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367823600,"objectID":"29fa1678e11ba90da425efaf83a0c644","permalink":"https://songshiyu01.github.io/publication/icra2013_monovo/","publishdate":"2013-05-06T00:00:00-07:00","relpermalink":"/publication/icra2013_monovo/","section":"publication","summary":"We present a real-time, accurate, large-scale monocular visual odometry system for real-world autonomous outdoor driving applications. The key contributions of our work are a series of architectural innovations that address the challenge of robust multithreading even for scenes with large motions and rapidly changing imagery. Our design is extensible for three or more parallel CPU threads. The system uses 3D-2D correspondences for robust pose estimation across all threads, followed by local bundle adjustment in the primary thread. In contrast to prior work, epipolar search operates in parallel in other threads to generate new 3D points at every frame. This significantly boosts robustness and accuracy, since only extensively validated 3D points with long tracks are inserted at keyframes. Fast-moving vehicles also necessitate immediate global bundle adjustment, which is triggered by our novel keyframe design in parallel with pose estimation in a thread-safe architecture. To handle inevitable tracking failures, a recovery method is provided. Scale drift is corrected only occasionally, using a novel mechanism that detects (rather than assumes) local planarity of the road by combining information from triangulated 3D points and the inter-image planar homography. Our system is optimized to output pose within 50 ms in the worst case, while average case operation is over 30 fps. Evaluations are presented on the challenging KITTI dataset for autonomous driving, where we achieve better rotation and translation accuracy than other state-of-the-art systems.","tags":[],"title":"Parallel, Real-Time Monocular Visual Odometry","type":"publication"},{"authors":null,"categories":null,"content":" Summary Scale drift is a crucial challenge for monocular autonomous driving to emulate the performance of stereo. This paper presents a real-time monocular SFM system that corrects for scale drift using a novel cue combination framework for ground plane estimation, yielding accuracy comparable to stereo over long driving sequences. Our ground plane estimation uses multiple cues like sparse features, dense inter-frame stereo and (when applicable) object detection. A data-driven mechanism is proposed to learn models from training data that relate observation covariances for each cue to error behavior of its underlying variables. During testing, this allows per-frame adaptation of observation covariances based on relative confidences inferred from visual data. Our framework significantly boosts not only the accuracy of monocular self-localization, but also that of applications like object localization that rely on the ground plane. Experiments on the KITTI dataset demonstrate the accuracy of our ground plane estimation, monocular SFM and object localization relative to ground truth, with detailed comparisons to prior art.\nAccuracy We demonstrate our performance on the KITTI dataset. For camera self-localization, our purely vision-based system achieves a rotation error of 0.005 degrees per meter and a translation error of 2.5%, which compares favorably even to state-of-the-art stereo systems and significantly outperforms other monocular systems. For 3D localization of other traffic participants like cars, we achieve low errors of 8% for near objects (within 30 meters) and 12% for far objects (beyond 30 meters).\nCue combination The main challenge in monocular SFM is scale drift, since unlike the case of stereo, there is no reference baseline. We overcome this challenge with a novel cue combination framework, that combines information from 3D points, inter-frame stereo and object detection.\nComparison to other systems Our real-time monocular SFM is comparable in accuracy to state-of-the-art stereo systems and significantly outperforms other monocular systems. A few example sequences are shown here from the KITTI benchmark.\nVideos These videos provide a 1-minute description of the paper and demonstrate the monocular visual odometry and 3D object localization results.\nOverview   3D object localization   Monocular visual odometry   ","date":1329552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1329552000,"objectID":"0ef0ca75b917dbadfe809e39d8154bc4","permalink":"https://songshiyu01.github.io/project/monocular_vo/","publishdate":"2012-02-18T00:00:00-08:00","relpermalink":"/project/monocular_vo/","section":"project","summary":"A real-time monocular visual odometry system that corrects for scale drift using a novel cue combination framework for ground plane estimation, yielding accuracy comparable to stereo over long driving sequences.","tags":[],"title":"Monocular Visual Odometry","type":"project"}]